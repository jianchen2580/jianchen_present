容器网络

DockerOne - Shanghai
23 June 2017

Tags: Docker, Networking

Jian Chen

* Overview

- 相关技术
- 容器网络
- Calico
- Contiv

* Linux虚拟网络设备

- Veth pair：一对虚拟网络设备，一端发送的数据会由另外一端接受，常用于不同网络命名空间通信
- TAP/TUN：用户态程序向内核协议栈注入数据的设备
- Iptables/Netfilter：Netfilter负责在内核中执行各种挂接的规则（过滤、修改、丢弃等），运行在内核模式中；Iptables模式是在用户模式下运行的进程，负责协助维护内核中Netfilter的各种规则表；通过二者的配合来实现整个Linux网络协议栈中灵活的数据包处理机制
- 网桥（docker0）: 网桥是一个二层网络设备，通过网桥可以将Linux支持的不同的端口连接起来
- 路由：Linux系统包含一个完整的路由功能，当IP层在处理数据发送或转发的时候，通过路由表来决定发往哪里

* Linux虚拟网络设备 - veth pair
一对虚拟网络设备，一端发送的数据会由另外一端接受，常用于不同网络命名空间通信
 
    # 创建network namespace
    ip netns add blue
    # list network namespace
    ip netns list

    # 创建veth pair
    ip link add veth0 type veth peer name veth1
    ip link list
    
    # 将veth1放入另一个netns
    ip link set veth1 netns blue
    ip link list
    ip netns exec blue ip link list

* Linux虚拟网络设备 - TAP／TUN

TAP/TUN设备是一种让用户态程序向内核协议栈注入数据的设备，TAP等同于一个以太网设备，工作在二层；而TUN则是一个虚拟点对点设备，工作在三层。

    ip tuntap add tap0 mode tap
    ip tuntap add tun0 mode tun
   
* Linux虚拟网络设备 - 网桥（Bridge）

网桥是一个二层设备，工作在链路层，主要是根据MAC学习来转发数据到不同的port。

    # 创建网桥
    brctl addbr br0
    # 添加设备到网桥
    brctl addif br0 eth1
    # 查询网桥mac表
    brctl showmacs br0

* Linux虚拟网络设备 - 路由

   ip route

* 相关技术术语

- IPAM：IP地址管理（分配），传统网络(DHCP)，容器网络IPAM，基于CIDR的IP地址段分配地或者精确为每一个容器分配IP
- Overlay：在现有二层或三层网络之上再构建起来一个独立的网络，这个网络通常会有自己独立的IP地址空间、交换或者路由的实现
- IPSesc：一个点对点的一个加密通信协议，一般会用到Overlay网络的数据通道里
- VXLAN：由VMware、Cisco、RedHat等联合提出的这么一个解决方案，这个解决方案最主要是解决VLAN支持虚拟网络数量（4096）过少的问题。支持1600万个虚拟网络
- BGP：主干网自治网络的路由协议，互联网由很多小的自治网络构成的，自治网络之间的三层路由由BGP实现
- SDN、Openflow：软件定义网络里面的一个术语，比如说我们经常听到的流表、控制平面，或者转发平面都是Openflow里的术语

* 容器网络需求

- 多主机容器互联
- 一容器一IP
- 网络隔离
- ACL
- SDN对接

* 容器网络

- Host Network
- CNI
- CNM
- Kubernetes网络

* 容器网络 - Host Network

容器共享Host的network namespace，使用宿主机的网络协议栈。不需要额外的配置，容器就可以共享宿主的各种网络资源。
优点:

- 简单，不需要任何额外配置
- 高效，没有NAT等额外的开销

缺点:

- 没有任何的网络隔离
- 容器和Host的端口号容易冲突
- 容器内任何网络配置都会影响整个宿主机

* 容器网络 - CNI

.image CNI_model.png _ 800

* 容器网络 - CNI

Container Network Interface (CNI)是由CoreOS发起的容器网络规范，是Kubernetes网络插件的基础。其基本思想为：Container Runtime在创建容器时，先创建好network namespace，然后调用CNI插件为这个netns配置网络，其后再启动容器内的进程。

CNI插件包括两部分：
IPAM Plugin负责给容器分配IP地址，主要实现包括host-local和dhcp。
CNI Plugin负责给容器配置网络，它包括两个基本的接口

- 配置网络: AddNetwork(net NetworkConfig, rt RuntimeConf) (types.Result, error)
- 清理网络: DelNetwork(net NetworkConfig, rt RuntimeConf) error

* 容器网络 - CNI

CNI只要求在容器创建时为容器分配网络资源、删除容器时释放网络资源:

.image CNI_call.png

CNI实现与外界的交互都通过进程参数和环境变量传递，也只要求输出结果符合CNI规范即可，与实现语言无关。
比如Calico早期版本就使用Python实现了CNI规范，为Kubernetes提供了网络实现。

* 容器网络 - CNI

常见的环境变量设置：

CNI_COMMAND：调用指定CNI动作，ADD表示增加网卡，DEL表示释放网卡
CNI_CONTAINERID：容器ID
CNI_NETNS：容器网络命名空间文件位置
CNI_ARGS：额外传递的参数
CNI_IFNAME：设置的容器网卡名称，如eth0

CNI插件均支持通过环境变量和标准输入传入参数：

    $ echo '{"cniVersion": "0.3.1","name": "mynet","type": "macvlan","bridge": "cni0","isGateway": true,"ipMasq": true,"ipam": {"type": "host-local","subnet": "10.244.1.0/24","routes": [{ "dst": "0.0.0.0/0" }]}}' | sudo CNI_COMMAND=ADD CNI_NETNS=/var/run/netns/a CNI_PATH=./bin CNI_IFNAME=eth0 CNI_CONTAINERID=a CNI_VERSION=0.3.1 ./bin/bridge
    $ echo '{"cniVersion": "0.3.1","type":"IGNORED", "name": "a","ipam": {"type": "host-local", "subnet":"10.1.2.3/24"}}' | sudo CNI_COMMAND=ADD CNI_NETNS=/var/run/netns/a CNI_PATH=./bin CNI_IFNAME=a CNI_CONTAINERID=a CNI_VERSION=0.3.1 ./bin/host-local

* 容器网络 - CNI

Bridge是最简单的CNI网络插件，它首先在Host创建一个网桥，然后再通过veth pair连接该网桥到container netns。
注意，Bridge模式下，多主机网络通信需要额外配置主机路由。可以借助Flannel或者Quagga动态路由等来自动配置。
配置示例:

    {
        "cniVersion": "0.3.0",
        "name": "mynet",
        "type": "bridge",
        "bridge": "mynet0",
        "isDefaultGateway": true,
        "forceAddress": false,
        "ipMasq": true,
        "hairpinMode": true,
        "ipam": {
            "type": "host-local",
            "subnet": "10.10.0.0/16"
        }
    }

* 容器网络 - CNI

DHCP插件是最主要的IPAM插件之一，用来通过DHCP方式给容器分配IP地址，在macvlan插件中也会用到DHCP插件。
在使用DHCP插件之前，需要先启动dhcp daemon:

    /opt/cni/bin/dhcp daemon &

然后配置网络使用dhcp作为IPAM插件

    {
        ...
        "ipam": {
            "type": "dhcp",
        }
    }

* 容器网络 - CNI

host-local是最常用的CNI IPAM插件，用来给container分配IP地址。
IPv4:

    {
        "ipam": {
            "type": "host-local",
            "subnet": "10.10.0.0/16",
            "rangeStart": "10.10.1.20",
            "rangeEnd": "10.10.3.50",
            "gateway": "10.10.0.254",
            "routes": [
                { "dst": "0.0.0.0/0" },
                { "dst": "192.168.0.0/16", "gw": "10.10.5.1" }
            ],
            "dataDir": "/var/my-orchestrator/container-ipam-state"
        }
    }

IPv6: 略

* 容器网络 - CNI

- Kubernetes
- Weave
- Macvlan
- Calico
- Flannel
- Contiv
- Mesos CNI

优点：

- CNI的优势是支持多种容器运行时，包括Docker、rkt、Mesos、Hyper等容器引擎都可以使用，而且社区活跃势头迅猛，Kubernetes加上CoreOS主推

缺点（也是优点）：

- 非Docker原生

* 容器网络 - CNM

.image cnm_model.jpg

Container network model (CNM)是Docker的网络模型，主要由以下3部分组成：

- Sandbox，包含容器网络栈的配置，包括Interface，路由表及DNS配置，对应的实现如：Linux Network Namespace；一个Sandbox可以包含多个Network；
- Endpoint，做为Sandbox接入Network的介质，对应的实现如：veth pair、TAP；一个Endpoint只能属于一个Network，也只能属于一个Sandbox；
- Network，一组可以相互通信的Endpoints；对应的实现如：Linux bridge、VLAN；Network有大量Endpoint资源组成。

* 容器网络 - CNM - Libnetwork

Libnetwork对于CNM的实现包括以下5类对象：

- NetworkController
- Network
- Driver
- Endpoint
- sandbox

* 容器网络 - CNM

- Docker Swarm overlay
- Macvlan & IP networkdrivers
- Calico
- Contiv
- Weave

Docker Libnetwork的优势就是原生，而且和Docker容器生命周期结合紧密；缺点也可以理解为是原生，被Docker“绑架”。

* 容器网络 - CNM

在CNM中，docker engine通过HTTP REST API调用网络实现，为容器配置网络。这些API接口涵盖网络管理、容器管理、创建endpoint等十几个接口。

* 容器网络 - CNM

- NetworkController，对外提供分配及管理网络的APIs，Docker Libnetwork支持多个活动的网络driver，NetworkController允许绑定特定的driver到指定的网络；
- Driver，网络驱动对用户而言是不直接交互的，它通过插件式的接入来提供最终网络功能的实现；Driver（包括IPAM）负责一个Network的管理，包括资源分配和回收。
具体细节移步https://github.com/docker/libnetwork/blob/master/docs/design.md

* Kubernetes网络

官方插件：

- kubenet：这是一个基于CNI bridge的网络插件（在bridge插件的基础上扩展了port mapping和traffic shaping），是目前推荐的默认插件
- CNI：CNI网络插件，需要用户将网络配置放到/etc/cni/net.d目录中，并将CNI插件的二进制文件放入/opt/cni/bin

第三方插件：

- Flannel
- Weave Net
- Calico
- OVS
- Contiv

* CNM & CNI阵营

CNM和CNI并不是网络实现，它们是网络规范和网络体系，关注的是网络管理问题
- CNM（Docker LibnetworkContainer Network Model） - Docker
- CNI（Container NetworkInterface）- Google（Kuberenetes）、CoreOS - kubernetes + CoreOS

* 容器网络方案

- 大二层
- NAT
- 隧道（Tunnel）方案 （Overlay VXLAN）
- 路由方案

* 大二层

- 利用三层网络中实现二层网络的扩展
- 利用三层网络避免维护庞大的mac和arp表，arp表无需泛洪到全网，TOR交换机维护本地mac地址表
- 二层过大导致的BUM（广播、单播、组播问题）：利用三层网络结合SDN避免

大二层，大二层是将物理网卡和容器网络桥接到同一个Linux Bridge，容器网络可以接入Linux Bridge，也可以将容器网络接入容器网桥docker0，再把docker0桥接到Linux Bridge；使得容器网络和宿主机网络在同一个二层网络。常见实现方案host、MacVLAN。

* NAT

NAT方式，NAT是默认的Docker网络，利用iptables地址转换实现宿主机IP到容器的通信。容器对外IP都是宿主机的IP，NAT的性能损耗比较大；但只要宿主机之间三层IP可达，容器之间就可以通信。

物理网卡和容器网络桥接到同一个Linux Bridge，容器网络可以接入Linux Bridge，也可以将容器网络接入容器网桥docker0，再把docker0桥接到Linux Bridge；使得容器网络和宿主机网络在同一个二层网络。常见实现方案：host，MacVLAN

* 容器网络方案 - 隧道方案（Overlay Networking）

目前在overlay技术领域有三大技术路线：VXLAN、NVGRE、STT，其中最火的就是VXLAN技术，包括vmware、cisco、h3c等多个虚拟化以及传统网络厂商，都是以VXLAN技术来构建自己的SDN解决方案。容器内基于overlay的解决方案有：

- Weave：UDP广播，本机建立新的BR，通过PCAP互通
- Open vSwitch（OVS）：基于VXLAN和GRE协议，但是性能方面损失比较严重
- Flannel：UDP广播，VXLAN
- Racher：IPsec

* 容器网络方案 - 隧道方案 - VXLAN

VXLAN报文格式

.image vxlan_frame_format.png _ 800

* 容器网络方案 - 隧道方案 - VXLAN

VXLAN报文是在原始的二层报文前面再封装一个新的报文，新的报文中和传统的以太网报文类似，拥有源目mac、源目ip等元组。当原始的二层报文来到vtep节点后会被封装上VXLAN包头（在VXLAN网络中把可以封装和解封装VXLAN报文的设备称为vtep，vtep可以是虚拟switch也可以是物理switch），打上VXLAN包头的报文到了目标的vtep后会将VXLAN包头解封装，并获取原始的二层报文。outer mac header以及outer ip header里面的相关元组信息都是vtep的信息，和原始的二层报文没有任何关系。所在数据包在源目vtep节点之间的传输和原始的二层报文是毫无关系的，依靠的是外层的包头完成。  
除此之外还有几个字段需要关注：

- 在VXLAN Header报文中封装了24bit的VXLAN ID（VNI）
- 在UDP header里面有一个source port的字段，用于VXLAN网络节点之间ECMP的hash；
- 在VXLAN Header里的reserved字段，作为保留字段，很多厂商都会加以运用来实现自己组网的一些特性。

* 容器网络方案 - 隧道方案 - VXLAN

二层报文转发

.image vxlan_layer2.png

Spine和Leaf节点之间是一个L3的网路，设备之间通过动态路由协议进行转发，并进行链路复杂。同一个vtep节点下，同一个subnet的两个主机直接进行转发，无需经过vtep去打VXLAN包头。不同的vtep下的同一个subnet通过VXLAN封装进行二层的扩展。

* 容器网络方案 - 隧道方案 - VXLAN

三层报文转发

.image vxlan_layer3.png

VXLAN的三层转发需要引入一个VXLAN三层网关的概念，两个不同subnet下的主机需要经过三层网关去路由.很多厂商的解决方案中三层网关集成在vtep上的，形成分布式网关的概念，所有的vtep上拥有所有subnet的网关，这样同一个vtep下主机间的流量无论是二层转发还是三层转发都会在本地完成.

* 容器网络方案 - 路由方案

- Calico：基于BGP协议的路由方案，支持很细致的ACL控制，对混合云亲和度比较高。
- Macvlan：从逻辑和Kernel层来看隔离性和性能最优的方案，基于二层隔离，所以需要二层交换机支持，大多数云服务商不支持，所以混合云上比较难以实现。

路由方案一般是从3层或者2层实现隔离和跨主机容器互通的，出了问题相对容易排查。
但是，Docker 1.9 以后，在决定容器网络方案还需考虑网络模型的”站队“问题，倒底是Docker的”CNM“，还是Google，CoreOS的”CNI“。

* Calico容器网络

Calico是一个纯3层的数据中心网络方案，而且无缝集成像OpenStack这种IaaS云架构，能够提供可控的VM、容器、裸机之间的IP通信。
CNM和CNI两大阵营都扮演着比较重要的角色。

.image calico.png

* Calico容器网络

- 每个计算节点利用Linux Kernel实现了一个高效的vRouter来负责数据转发
- vRouter通过BGP协议负责把自己上运行的workload的路由信息像整个Calico网络内传播
- 小规模部署可以直接互联，大规模下可通过指定的BGP route reflector来完成。
- 基于iptables提供了丰富而灵活的网络Policy，保证通过各个节点上的ACLs来提供Workload的多租户隔离、安全组以及其他可达性限制等功能。

* Calico核心组件

.image calico_core.png _ 400
- Felix，Calico Agent，跑在每台需要运行Workload的节点上，主要负责配置路由及ACLs等信息来确保Endpoint的连通状态；
- etcd，分布式键值存储，主要负责网络元数据一致性，确保Calico网络状态的准确性；
- BGP Client（BIRD）, 主要负责把Felix写入Kernel的路由信息分发到当前Calico网络，确保Workload间的通信的有效性；
- BGP Route Reflector（BIRD），大规模部署时使用，摒弃所有节点互联的 mesh 模式，通过一个或者多个BGP Route Reflector来完成集中式的路由分发。

* Calico - IP结构

.image calico_ippack.png _ 600
Calico节点组网可以直接利用数据中心的网络结构（无论是L2或者L3），不需要额外的NAT，隧道或者Overlay Network。 
没有封包解包，节约CPU计算资源,提高网络的性能。

* Calico - IP路由

workload之间的数据流量都是通过IP路由的方式完成互联

.image calico_ip.png _ 900

* Calico - IP路由实现

.image calico_ip_realize.png _ 900

* Calico - 安全策略ACL

.image calico_acl.png _ 900
- ACLs Profile只要依靠iptables和ipset，可提供针对每个容器级别的规则定义
- 通过iptabls查看对应的chain和filter

* Calico - L2全联接

.image calico_l2.png _ 700

* Calico - L3联接

.image calico_l3.png _ 700

* Contiv Netplugin

Contiv Netplugin 是来自思科的解决方案。编程语言为 Go。它基于 OpenvSwitch，以插件化的形式支持容器访问网络，支持 VLAN，Vxlan，多租户，主机访问控制策略等。作为思科整体支持容器基础设施contiv项目的网络部分，最大的亮点在于容器被赋予了 SDN 能力，实现对容器更细粒度，更丰富的访问控制功能。另外，对 Docker CNM 网络模型的支持，并内置了 IPAM 接口，不仅仅提供了一容器一 IP，而且容器的网络信息被记录的容器配置中，伴随着容器的整个生命周期，减低因为状态不同步造成网络信息丢失的风险。有别于 CNI，这种内聚化的设计有利于减少对第三方模块的依赖。

* Contiv Netplugin核心组件

.image contriv_core.png _ 900

* Contiv Netplugin核心组件

- Netmaster 后台进程负责记录所有节点状态，保存网络信息，分配 IP 地址
- Netplugin 后台进程作为每个宿主机上的 Agent 与 Docker 及 OVS 通信，处理来自 Docker 的请求，管理 OVS。Docker 方面接口为 remote driver，包括一系列 Docker 定义的 JSON-RPC(POST) 消息。OVS 方面接口为 remote ovsdb，也是 JSON-RPC 消息。以上消息都在 localhost 上处理。
- 集群管理依赖 etcd/serf


* Contiv Netplugin 代码结构

.image contriv_code.png _ 800

* Calico Vs Contiv

* Calico Vs Contiv

.image calico_vs_contiv1.png _ 800

* Calico Vs Contiv

.image calico_vs_contiv2.png _ 800

* Calico Vs Contiv - 包转发 

.image calico_vs_contiv_package.png _ 800

* Calico的优势

- 网络拓扑直观易懂，平行式扩展，可扩展性强
- 容器间网络三层隔离，无需要担心arp风暴
- 基于iptable/linux kernel包转发效率高，损耗低
- 社区活跃，正式版本较成熟

* Netplugin的优势

- 较早支持CNM模型。与已有的网络基础设施兼容性较高，改造影响小。基于VLAN的平行扩展与现有网络结构地位对等
- SDN能力，能够对容器的网络访问做更精细的控制
- 多租户支持，具备未来向混合云/公有云迁移的潜力
- 代码规模不大，逻辑结构清晰，并发好，VLAN在公司内部有开发部署运维实践经验，稳定性经过生产环境验证
- 京东基于相同的技术栈（OVS + VLAN）已支持10w+ 容器的运行。

* Q&A

.image gopher-mat.png

