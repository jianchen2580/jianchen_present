Docker网络
23 June 2017
Tags: Docker, Network

Jian Chen

* Overview

- 相关技术
- 容器网络方案
- 容器安全
- CNM & CNI阵营
- Calico
- Contiv

* Linux虚拟网络设备

- 网络的命名空间（namespace): 容器间的网络隔离
- Veth pair：一对虚拟网络设备，一端发送的数据会由另外一端接受，常用于不同网络命名空间通信
- TAP/TUN：用户态程序向内核协议栈注入数据的设备
- Iptables/Netfilter：Netfilter负责在内核中执行各种挂接的规则（过滤、修改、丢弃等），运行在内核模式中；Iptables模式是在用户模式下运行的进程，负责协助维护内核中Netfilter的各种规则表；通过二者的配合来实现整个Linux网络协议栈中灵活的数据包处理机制
- 网桥（docker0）: 网桥是一个二层网络设备，通过网桥可以将Linux支持的不同的端口连接起来
- 路由：Linux系统包含一个完整的路由功能，当IP层在处理数据发送或转发的时候，通过路由表来决定发往哪里

* 相关技术术语 - 网桥（Bridge）

    # 创建网桥
    brctl addbr br0
    # 添加设备到网桥
    brctl addif br0 eth1
    # 查询网桥mac表
    brctl showmacs br0

* 相关技术术语 - veth pair

    # 创建veth pair
    ip link add veth0 type veth peer name veth1
    
    # 将veth1放入另一个netns
    ip link set veth1 netns newns

* 相关技术术语 - TAP／TUN

    ip tuntap add tap0 mode tap
    ip tuntap add tun0 mode tun
    
* 相关技术术语

- IPAM：IP地址管理（分配），传统网络(DHCP)，容器网络IPAM，基于CIDR的IP地址段分配地或者精确为每一个容器分配IP
- Overlay：在现有二层或三层网络之上再构建起来一个独立的网络，这个网络通常会有自己独立的IP地址空间、交换或者路由的实现
- IPSesc：一个点对点的一个加密通信协议，一般会用到Overlay网络的数据通道里
- VXLAN：由VMware、Cisco、RedHat等联合提出的这么一个解决方案，这个解决方案最主要是解决VLAN支持虚拟网络数量（4096）过少的问题。支持1600万个虚拟网络
- BGP：主干网自治网络的路由协议，互联网由很多小的自治网络构成的，自治网络之间的三层路由由BGP实现
- SDN、Openflow：软件定义网络里面的一个术语，比如说我们经常听到的流表、控制平面，或者转发平面都是Openflow里的术语

* 容器网络需求

- 一容器一IP
- 多主机容器互联
- 网络隔离
- ACL
- 对接SDN
- ...

* 容器网络

- Host Network
- CNI
- CNM
- Kubernetes网络

* 容器网络 - Host Network

容器共享Host的network namespace，使用宿主机的网络协议栈。不需要额外的配置，容器就可以共享宿主的各种网络资源。
优点
- 简单，不需要任何额外配置
- 高效，没有NAT等额外的开销
缺点
- 没有任何的网络隔离
- 容器和Host的端口号容易冲突
- 容器内任何网络配置都会影响整个宿主机

* 容器网络 - CNI

Container Network Interface (CNI)是由CoreOS发起的容器网络规范，是Kubernetes网络插件的基础。其基本思想为：Container Runtime在创建容器时，先创建好network namespace，然后调用CNI插件为这个netns配置网络，其后再启动容器内的进程。

CNI插件包括两部分：
CNI Plugin负责给容器配置网络，它包括两个基本的接口
  配置网络: AddNetwork(net NetworkConfig, rt RuntimeConf) (types.Result, error)
  清理网络: DelNetwork(net NetworkConfig, rt RuntimeConf) error
IPAM Plugin负责给容器分配IP地址，主要实现包括host-local和dhcp。

* 容器网路 - CNI

.image CNI_model.png

* 容器网路 - CNM

.image cnm_model.jpg

- Sandbox，包含容器网络栈的配置，包括Interface，路由表及DNS配置，对应的实现如：Linux Network Namespace；一个Sandbox可以包含多个Network；
- Endpoint，做为Sandbox接入Network的介质，对应的实现如：veth pair、TAP；一个Endpoint只能属于一个Network，也只能属于一个Sandbox；
- Network，一组可以相互通信的Endpoints；对应的实现如：Linux bridge、VLAN；Network有大量Endpoint资源组成。

* 容器网路 - CNM - Libnetwork

Libnetwork对于CNM的实现包括以下5类对象：
- NetworkController
- Network
- Driver
- Endpoint
- sandbox

* 容器网络方案

- 大二层
- NAT
- 隧道（Tunnel）方案 （Overlay VXLAN）
- 路由方案

* 大二层 + NAT

大二层，大二层是将物理网卡和容器网络桥接到同一个Linux Bridge，容器网络可以接入Linux Bridge，也可以将容器网络接入容器网桥docker0，再把docker0桥接到Linux Bridge；使得容器网络和宿主机网络在同一个二层网络。常见实现方案host、MacVLAN。

NAT方式，NAT是默认的Docker网络，利用iptables地址转换实现宿主机IP到容器的通信。容器对外IP都是宿主机的IP，NAT的性能损耗比较大；但只要宿主机之间三层IP可达，容器之间就可以通信。

物理网卡和容器网络桥接到同一个Linux Bridge，容器网络可以接入Linux Bridge，也可以将容器网络接入容器网桥docker0，再把docker0桥接到Linux Bridge；使得容器网络和宿主机网络在同一个二层网络。常见实现方案
- host
- MacVLAN

* 容器网络方案 - 隧道方案（Overlay Networking）

- Weave：UDP广播，本机建立新的BR，通过PCAP互通
- Open vSwitch（OVS）：基于VXLAN和GRE协议，但是性能方面损失比较严重
- Flannel：UDP广播，VXLAN
- Racher：IPsec

隧道方案在IaaS层的网络中应用较多，随着节点规模的增长复杂度会提升，而且出了网络问题跟踪起来比较麻烦，大规模集群情况下需要考虑


* 容器网络方案 - 路由方案

- Calico：基于BGP协议的路由方案，支持很细致的ACL控制，对混合云亲和度比较高。
- Macvlan：从逻辑和Kernel层来看隔离性和性能最优的方案，基于二层隔离，所以需要二层交换机支持，大多数云服务商不支持，所以混合云上比较难以实现。

路由方案一般是从3层或者2层实现隔离和跨主机容器互通的，出了问题相对容易排查。
但是，Docker 1.9 以后，在决定容器网络方案还需考虑网络模型的”站队“问题，倒底是Docker的”CNM“，还是Google，CoreOS的”CNI“。

* CNM & CNI阵营

CNM和CNI并不是网络实现，它们是网络规范和网络体系，关注的是网络管理问题
- CNM（Docker LibnetworkContainer Network Model） - Docker
- CNI（Container NetworkInterface）- Google（Kuberenetes）、CoreOS

* CNM

- Docker Swarm overlay
- Macvlan & IP networkdrivers
- Calico
- Contiv
- Weave

Docker Libnetwork的优势就是原生，而且和Docker容器生命周期结合紧密；缺点也可以理解为是原生，被Docker“绑架”。

* CNM Docker网络管理

- NetworkController，对外提供分配及管理网络的APIs，Docker Libnetwork支持多个活动的网络driver，NetworkController允许绑定特定的driver到指定的网络；
- Driver，网络驱动对用户而言是不直接交互的，它通过插件式的接入来提供最终网络功能的实现；Driver（包括IPAM）负责一个Network的管理，包括资源分配和回收。
具体细节移步https://github.com/docker/libnetwork/blob/master/docs/design.md

* CNI

- Kubernetes
- Weave
- Macvlan
- Calico
- Flannel
- Contiv
- Mesos CNI

CNI的优势是兼容其他容器技术（e.g. rkt）及上层编排系统（Kubernetes & Mesos)，而且社区活跃势头迅猛，Kubernetes加上CoreOS主推；缺点（也是优点）是非Docker原生

* Calico容器网络

Calico是一个纯3层的数据中心网络方案，而且无缝集成像OpenStack这种IaaS云架构，能够提供可控的VM、容器、裸机之间的IP通信。
CNM和CNI两大阵营都扮演着比较重要的角色。

.image calico.png

* Calico容器网络

- 每个计算节点利用Linux Kernel实现了一个高效的vRouter来负责数据转发
- vRouter通过BGP协议负责把自己上运行的workload的路由信息像整个Calico网络内传播
- 小规模部署可以直接互联，大规模下可通过指定的BGP route reflector来完成。
- 基于iptables提供了丰富而灵活的网络Policy，保证通过各个节点上的ACLs来提供Workload的多租户隔离、安全组以及其他可达性限制等功能。

* Calico - IP结构

.image calico_ippack.png _ 600
Calico节点组网可以直接利用数据中心的网络结构（无论是L2或者L3），不需要额外的NAT，隧道或者Overlay Network。 
没有封包解包，节约CPU计算资源,提高网络的性能。

* Calico - IP路由

workload之间的数据流量都是通过IP路由的方式完成互联

.image calico_ip.png _ 900

* Calico - IP路由实现

.image calico_ip_relize.png

* Calico - 安全策略ACL

.image calico_acl.
- ACLs Profile只要依靠iptables和ipset，可提供针对每个容器级别的规则定义
- 通过iptabls查看对应的chain和filter

* Calico核心组件

.image calico_core.png _ 400
- Felix，Calico Agent，跑在每台需要运行Workload的节点上，主要负责配置路由及ACLs等信息来确保Endpoint的连通状态；
- etcd，分布式键值存储，主要负责网络元数据一致性，确保Calico网络状态的准确性；
- BGP Client（BIRD）, 主要负责把Felix写入Kernel的路由信息分发到当前Calico网络，确保Workload间的通信的有效性；
- BGP Route Reflector（BIRD），大规模部署时使用，摒弃所有节点互联的 mesh 模式，通过一个或者多个BGP Route Reflector来完成集中式的路由分发。

* Calico L2全联接

.image calico_l2.png _ 700

* Calico L3

.image calico_l3.png _ 700

* Contiv Netplugin

Contiv Netplugin 是来自思科的解决方案。编程语言为 Go。它基于 OpenvSwitch，以插件化的形式支持容器访问网络，支持 VLAN，Vxlan，多租户，主机访问控制策略等。作为思科整体支持容器基础设施contiv项目的网络部分，最大的亮点在于容器被赋予了 SDN 能力，实现对容器更细粒度，更丰富的访问控制功能。另外，对 Docker CNM 网络模型的支持，并内置了 IPAM 接口，不仅仅提供了一容器一 IP，而且容器的网络信息被记录的容器配置中，伴随着容器的整个生命周期，减低因为状态不同步造成网络信息丢失的风险。有别于 CNI，这种内聚化的设计有利于减少对第三方模块的依赖。

* Contiv Netplugin核心组件

.image contriv_core.png

* Contiv Netplugin核心组件

- Netmaster 后台进程负责记录所有节点状态，保存网络信息，分配 IP 地址
- Netplugin 后台进程作为每个宿主机上的 Agent 与 Docker 及 OVS 通信，处理来自 Docker 的请求，管理 OVS。Docker 方面接口为 remote driver，包括一系列 Docker 定义的 JSON-RPC(POST) 消息。OVS 方面接口为 remote ovsdb，也是 JSON-RPC 消息。以上消息都在 localhost 上处理。
- 集群管理依赖 etcd/serf


* Contiv Netplugin 代码结构

.image contriv_code.png _ 500

* Calico Vs Contiv

.image calico_vs_contiv1.png

* Calico Vs Contiv

.image calico_vs_contiv2.png

* Calico Vs Contiv - 包转发

.image calico_vs_contiv_package.png

* Calico的优势

- 网络拓扑直观易懂，平行式扩展，可扩展性强
- 容器间网络三层隔离，无需要担心arp风暴
- 基于iptable/linux kernel包转发效率高，损耗低
- 社区活跃，正式版本较成熟

* Netplugin的优势

- 较早支持CNM模型。与已有的网络基础设施兼容性较高，改造影响小。基于VLAN的平行扩展与现有网络结构地位对等
- SDN能力，能够对容器的网络访问做更精细的控制
- 多租户支持，具备未来向混合云/公有云迁移的潜力
- 代码规模不大，逻辑结构清晰，并发好，VLAN在公司内部有开发部署运维实践经验，稳定性经过生产环境验证
- 京东基于相同的技术栈（OVS + VLAN）已支持10w+ 容器的运行。

* 性能对比
